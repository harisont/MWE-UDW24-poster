\documentclass{guposter}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[normalem]{ulem}
\usepackage[inline]{enumitem}
\setlist{noitemsep} 
\usepackage{booktabs}
\usepackage{caption}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage[table,xcdraw]{xcolor}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}
\usepackage{stfloats}
\usepackage{makecell}

\title{\parbox{\linewidth}{Synthetic-Error Augmented Parsing of Swedish \newline as a Second Language: Experiments with Word Order}}
\author{Arianna Masciolini, Emilie Marie Carreau Francis and Maria Irena Szawerna}
\institute{Språkbanken Text, Department of Swedish, Multilingualism, Language Technology, University of Gothenburg}


\begin{document}
\maketitle

% Tip: for long titles, use \parbox{\linewidth}{text}

\block{Abstract}{
Ungrammatical text poses significant challenges for dependency parsers. 
In this paper, we explore the effectiveness of using synthetic data to improve performance on essays written by learners of Swedish as a second language. 
Due to their relevance and ease of annotation, we restrict our initial experiments to word order errors.
To do that, we build a corrupted version of the standard Swedish Universal Dependencies (UD) treebank Talbanken, mimicking the error patterns and frequency distributions observed in the Swedish Learner Language (SweLL) corpus.
We then use the MaChAmp (Massive Choice, Ample tasks) toolkit to train an array of BERT-based dependency parsers, fine-tuning on different combinations of original and corrupted data. 
We evaluate the resulting models not only on their respective test sets but also, most importantly, on a smaller collection of sentence-correction pairs derived from SweLL. 
Results show small but significant performance improvements on the target domain, with minimal decline on normative data.

\bigskip

\color{gublue}{\textbf{Keywords}}: \color{black}{\textbf{Dependency Parsing, Data Augmentation, Second Language Acquisition, L2 Swedish}}
}
\begin{columns}
\column{0.5}

\block{Background}{
\begin{itemize}
    \item \textbf{ungrammatical text} is challenging for automatic annotation tools
    \item the lack of manually annotated L2 corpora makes it hard to train \newline domain-specific models
    \item experiments with \textbf{synthetically generated errors}, focusing on word order
\end{itemize}

\bigskip

\setlength{\unitlength}{0.65mm}
\hspace{1em}
\begin{picture}(562.0,110.0)
\thicklines
  \put(0.0,0.0){Skattebrott}
  \put(0.0,-15.0){\textit{Tax offences}}
  \put(109.0,0.0){påverkar}
  \put(109.0,-15.0){\textit{affect}}
  \put(191.0,0.0){\textbf{negativt}}
  \put(191.0,-15.0){\textit{\textbf{negatively}}}
  \put(273.0,0.0){\textbf{samhället}}
  \put(273.0,-15.0){\textit{\textbf{society}}}
  \put(364.0,0.0){på}
  \put(364.0,-15.0){\textit{in}}
  \put(401.0,0.0){många}
  \put(401.0,-15.0){\textit{many}}
  \put(456.0,0.0){ sätt}
  \put(456.0,-15.0){ \textit{ways}}
  \put(0.0,15.0){{\tiny NOUN}}
  \put(109.0,15.0){{\tiny VERB}}
  \put(191.0,15.0){{\tiny ADV}}
  \put(273.0,15.0){{\tiny NOUN}}
  \put(364.0,15.0){{\tiny ADP}}
  \put(401.0,15.0){{\tiny ADJ}}
  \put(456.0,15.0){{\tiny NOUN}}
  \put(64.5,30.0){\oval(106.24770642201835,33.333333333333336)[t]}
  \put(11.376146788990823,35.0){\vector(0,-1){5.0}}
  \put(49.5,49.66666666666667){{\tiny nsubj}}
  \put(242.0,30.0){\oval(78.34146341463415,33.333333333333336)[t]}
  \put(202.8292682926829,35.0){\vector(0,-1){5.0}}
  \put(227.0,49.66666666666667){{\tiny amod}}
  \put(211.0,30.0){\oval(162.17073170731706,66.66666666666667)[t]}
  \put(292.0853658536585,35.0){\vector(0,-1){5.0}}
  \put(196.0,66.33333333333334){{\tiny obj}}
  \put(420.0,30.0){\oval(88.73913043478261,66.66666666666667)[t]}
  \put(375.6304347826087,35.0){\vector(0,-1){5.0}}
  \put(405.0,66.33333333333334){{\tiny case}}
  \put(438.5,30.0){\oval(49.54545454545455,33.333333333333336)[t]}
  \put(413.72727272727275,35.0){\vector(0,-1){5.0}}
  \put(423.5,49.66666666666667){{\tiny amod}}
  \put(302.5,30.0){\oval(346.13544668587895,100.0)[t]}
  \put(475.5677233429395,35.0){\vector(0,-1){5.0}}
  \put(287.5,83.0){{\tiny obl}}
  \put(124.0,110.0){\vector(0,-1){80.0}}
  \put(129.0,100.0){{\tiny root}}
\end{picture}
\vspace{0.25cm}
}

\block{Data}{
\begin{minipage}{\linewidth}
    \centering
    \resizebox{0.7\columnwidth}{!}{
    \begin{tabular}{r|c|ccc}
        {} & \textbf{Description} & \textbf{Train} & \textbf{Dev} & \textbf{Test} \\ \hline
        \textbf{SweLL} & L2 Swedish & - & - & 69 \\
        \textbf{Talbanken} & standard Swedish & 4303 & 504 & 1219\\
        \textbf{Corrupted} & synthetic errors & 4303 & 504 & 1219\\
    \end{tabular}}
    \label{tab:datasizes}
\end{minipage}

\vskip 30pt

\begin{itemize}
    \item manually annotated \textbf{SweLL} subset with word order errors only
    \item reference \textbf{Talbanken} treebank
    \item automatically \textbf{corrupted} version of Talbanken:
    \begin{itemize}
        \item \textbf{S-Adv} (misplaced adverbial) errors obtained by swapping adverbial subtrees with their syntactic heads
        \item \textbf{S-FinV} (misplaced finite verb) error obtained by swapping subject subtrees with their syntactic heads
        \item other types of errors (\textbf{S-WO}) approximated by swapping pairs of random \newline subsequent tokens
    \end{itemize}
\end{itemize}

\setlength{\unitlength}{0.60mm}
\begin{picture}(696.0,130.0)
\thicklines
\centering
  \put(0.0,0.0){\small Sakta}
  \put(0.0,-15){\small \textit{Slowly}}
  \put(55.0,0.0){\small och}
  \put(55.0,-15){\small \textit{and}}
  \put(110.0,0.0){\small kanske}
  \put(110.0,-15){\small \textit{perhaps}}
  \put(174.0,0.0){\small \underline{avsaktande}}
  \put(174.0,-15){\small \textit{\underline{slowing down}}}
  \put(274.0,0.0){\small \textbf{rent}}
  \put(287.0,-15){\small \textit{\textbf{even}}}
  \put(320.0,0.0){\small \textbf{av}}
  \put(357.0,0.0){\small är}
  \put(357,-15){\small \textit{is}}
  \put(394.0,0.0){\small det}
  \put(394,-15){\small \textit{the}}
  \put(431.0,0.0){\small rätta}
  \put(431.0,-15){\small\textit{correct}}
  \put(486.0,0.0){\small ordet}
  \put(486.0,-15){\small \textit{word}}
  \put(541.0,0.0){\small .}
  \put(541.0,-15){\small \textit{.}}
  \put(0.0,15.0){{\tiny ADJ}}
  \put(55.0,15.0){{\tiny CCONJ}}
  \put(110.0,15.0){{\tiny ADV}}
  \put(174.0,15.0){{\tiny ADJ}}
  \put(274.0,15.0){{\tiny ADV}}
  \put(320.0,15.0){{\tiny ADP}}
  \put(357.0,15.0){{\tiny AUX}}
  \put(394.0,15.0){{\tiny DET}}
  \put(431.0,15.0){{\tiny ADJ}}
  \put(486.0,15.0){{\tiny NOUN}}
  \put(541.0,15.0){{\tiny PUNCT}}
  \put(253.0,30.0){\oval(485.38271604938274,133.33333333333334)[t]}
  \put(10.308641975308632,35.0){\vector(0,-1){5.0}}
  \put(238.0,99.66666666666667){{\tiny nsubj}}
  \put(124.5,30.0){\oval(116.47899159663865,66.66666666666667)[t]}
  \put(66.26050420168067,35.0){\vector(0,-1){5.0}}
  \put(109.5,66.33333333333334){{\tiny cc}}
  \put(152.0,30.0){\oval(59.3125,33.333333333333336)[t]}
  \put(122.34375,35.0){\vector(0,-1){5.0}}
  \put(137.0,49.66666666666667){{\tiny advmod}}
  \put(107.0,30.0){\oval(172.27586206896552,100.0)[t]}
  \put(193.13793103448276,35.0){\vector(0,-1){5.0}}
  \put(92.0,83.0){{\tiny conj}}
  \put(244.0,30.0){\oval(97.0,33.333333333333336)[t]}
  \put(292.5,35.0){\vector(0,-1){5.0}}
  \put(229.0,49.66666666666667){{\tiny \textbf{advmod}}}
  \put(317.0,30.0){\oval(39.47826086956522,33.333333333333336)[t]}
  \put(336.7391304347826,35.0){\vector(0,-1){5.0}}
  \put(302.0,49.66666666666667){{\tiny fixed}}
  \put(431.5,30.0){\oval(126.67441860465117,100.0)[t]}
  \put(368.16279069767444,35.0){\vector(0,-1){5.0}}
  \put(416.5,83.0){{\tiny cop}}
  \put(450.0,30.0){\oval(88.73913043478261,66.66666666666667)[t]}
  \put(405.6304347826087,35.0){\vector(0,-1){5.0}}
  \put(435.0,66.33333333333334){{\tiny det}}
  \put(468.5,30.0){\oval(49.54545454545455,33.333333333333336)[t]}
  \put(443.72727272727275,35.0){\vector(0,-1){5.0}}
  \put(453.5,49.66666666666667){{\tiny amod}}
  \put(533.5,30.0){\oval(49.54545454545455,33.333333333333336)[t]}
  \put(558.2727272727273,35.0){\vector(0,-1){5.0}}
  \put(518.5,49.66666666666667){{\tiny punct}}
  \put(501.0,130.0){\vector(0,-1){100.0}}
  \put(506.0,120.0){{\tiny root}}\end{picture}

\centering \vspace{2cm}
\textbf{S-Adv}

\setlength{\unitlength}{0.46mm}
\vspace{1cm}
\begin{picture}(880.0,130.0)
\thicklines
  \put(0.0,0.0){\hspace{0.65em} \small \textbf{Vi}}
  \put(0.0,-15){\hspace{-0.75em}\small \textit{(Do)} \small\textit{\textbf{we}}}
  \put(46.0,0.0){\small \underline{har}}
  \put(46.0,-15){\small \textit{\underline{have}}}
  \put(92.0,0.0){\small en}
  \put(92.0,-15){\small \textit{a}}
  \put(129.0,0.0){\small tendens}
  \put(129,-15){\small \textit{tendency}}
  \put(202.0,0.0){\small att}
  \put(202,-15){\small \textit{to}}
  \put(248.0,0.0){\small alltmer}
  \put(232,-15){\small \textit{increasingly}}
  \put(321.0,0.0){\small spalta}
  \put(332,-15){\small \textit{segregate}}
  \put(385.0,0.0){\small upp}
  \put(422.0,0.0){\small oss}
  \put(422,-15){\small \textit{us}}
  \put(468.0,0.0){\small och}
  \put(468,-15){\small \textit{and}}
  \put(523.0,0.0){\small leva}
  \put(523,-15){\small \textit{live}}
  \put(569.0,0.0){\small generationsvis}
  \put(569.0,-15){\small \textit{generation-wise}}
  \put(705.0,0.0){\small ?}
  \put(705.0,-15){\small \textit{?}}
  \put(0.0,15.0){{\hspace{0.25em} \tiny PRON}}
  \put(46.0,15.0){{\tiny VERB}}
  \put(92.0,15.0){{\tiny DET}}
  \put(129.0,15.0){{\tiny NOUN}}
  \put(202.0,15.0){{\tiny PART}}
  \put(248.0,15.0){{\tiny ADV}}
  \put(321.0,15.0){{\tiny VERB}}
  \put(385.0,15.0){{\tiny ADV}}
  \put(422.0,15.0){{\tiny PRON}}
  \put(468.0,15.0){{\tiny CCONJ}}
  \put(523.0,15.0){{\tiny VERB}}
  \put(569.0,15.0){{\tiny ADV}}
  \put(705.0,15.0){{\tiny PUNCT}}
  \put(33.0,30.0){\oval(39.47826086956522,33.333333333333336)[t]}
  \put(13.26086956521739,35.0){\vector(0,-1){5.0}}
  \put(18.0,49.66666666666667){{\tiny \textbf{nsubj}}}
  \put(120.5,30.0){\oval(28.89189189189189,33.333333333333336)[t]}
  \put(106.05405405405405,35.0){\vector(0,-1){5.0}}
  \put(105.5,49.66666666666667){{\tiny det}}
  \put(107.5,30.0){\oval(79.3855421686747,66.66666666666667)[t]}
  \put(147.19277108433735,35.0){\vector(0,-1){5.0}}
  \put(92.5,66.33333333333334){{\tiny obj}}
  \put(271.5,30.0){\oval(116.47899159663865,66.66666666666667)[t]}
  \put(213.26050420168067,35.0){\vector(0,-1){5.0}}
  \put(256.5,66.33333333333334){{\tiny mark}}
  \put(294.5,30.0){\oval(68.89041095890411,33.333333333333336)[t]}
  \put(260.05479452054794,35.0){\vector(0,-1){5.0}}
  \put(279.5,49.66666666666667){{\tiny advmod}}
  \put(245.0,30.0){\oval(190.4375,100.0)[t]}
  \put(340.21875,35.0){\vector(0,-1){5.0}}
  \put(230.0,83.0){{\tiny acl}}
  \put(373.0,30.0){\oval(59.3125,33.333333333333336)[t]}
  \put(402.65625,35.0){\vector(0,-1){5.0}}
  \put(358.0,49.66666666666667){{\tiny compound}}
  \put(391.5,30.0){\oval(98.02970297029702,66.66666666666667)[t]}
  \put(440.5148514851485,35.0){\vector(0,-1){5.0}}
  \put(376.5,66.33333333333334){{\tiny obj}}
  \put(505.5,30.0){\oval(49.54545454545455,33.333333333333336)[t]}
  \put(480.72727272727275,35.0){\vector(0,-1){5.0}}
  \put(490.5,49.66666666666667){{\tiny cc}}
  \put(442.0,30.0){\oval(200.5148514851485,100.0)[t]}
  \put(542.2574257425742,35.0){\vector(0,-1){5.0}}
  \put(427.0,83.0){{\tiny conj}}
  \put(566.0,30.0){\oval(39.47826086956522,33.333333333333336)[t]}
  \put(585.7391304347826,35.0){\vector(0,-1){5.0}}
  \put(551.0,49.66666666666667){{\tiny advmod}}
  \put(395.5,30.0){\oval(658.5447647951441,133.33333333333334)[t]}
  \put(724.7723823975721,35.0){\vector(0,-1){5.0}}
  \put(380.5,99.66666666666667){{\tiny punct}}
  \put(61.0,130.0){\vector(0,-1){100.0}}
  \put(66.0,120.0){{\tiny root}}
\end{picture}
\centering \vskip 35pt
\textbf{S-FinV}
}

\block{}{
    \begin{center}
        \includegraphics[width=0.182\textwidth]{images/qr_codes}
    \end{center}
}

\column{0.5}

\block{Models}{
\begin{itemize}
    \item fine tuning a \textbf{Swedish BERT} model with the \textbf{MaChAmp toolkit}
    \item experiments with various data configurations:
    \begin{itemize}
        \item \textbf{baseline} trained on the standard Talbanken
        \item corrupted data with normative Swedish to mimic the \textbf{real-world error \newline frequency}
        \item \textbf{50-50 ratio} of corrupted and normative sentences
        \item experiments with \textbf{sequential training}: the baseline further fine-tuned for 10 or 20 extra epochs on purely synthetic error data 
    \end{itemize}
\end{itemize}
\vskip 1em
\begin{minipage}{\linewidth}
    \centering
    \resizebox{0.55\columnwidth}{!}{
    \normalsize
        \begin{tabular}{cr|c|c}
        \multicolumn{2}{c|}{\normalsize\textbf{Name}} & \textbf{\small \% \normalsize Normative} & \textbf{\small \% \normalsize Errors} \\ \hline
        \multicolumn{2}{c|}{baseline} & 100 & 0\\ 
        \multicolumn{2}{c|}{mix15} & 85 & 15 \\ 
        \multicolumn{2}{c|}{mix50} & 50 & 50 \\ 
        \hline
        \multirow{2}{3em}{seq10} & \small{(step 1)} & 100 & 0 \\
         & \small{(step 2)} & 0 & 100  \\
        \multirow{2}{3em}{seq20} & \small{(step 1)} & 100 & 0 \\
        & \small{(step 2)} & 0 & 100  \\
    \end{tabular}}
\end{minipage}
}
\block{Results and Conclusions}{
\begin{minipage}{\linewidth}
    \centering
    \resizebox{0.645\columnwidth}{!}{
    \begin{tabular}{ccccccc}
\textbf{}         & \multicolumn{2}{c}{\textbf{Talbanken}}                                               & \multicolumn{2}{c}{\textbf{Corrupted}}                        & \multicolumn{2}{c}{\textbf{SweLL}}                            \\
                  & \textbf{LAS}                                         & \textbf{UAS}                  & \textbf{LAS}                  & \textbf{UAS}                  & \textbf{LAS}                  & \textbf{UAS}                  \\ \Xhline{2.5\arrayrulewidth}
baseline & {92.42}                         & {94.30}  & {80.20}  & {83.29}  & {88.28}  & {91.16}  \\ \Xhline{2.5\arrayrulewidth}
mix15    & 92.23                                                & 94.05                         & \cellcolor{Gray}87.96 & \cellcolor{Gray}90.50 & 87.63                         & 90.60                         \\ \hline
mix50    & \cellcolor{Gray}91.54 & \cellcolor{Gray}93.58 & \cellcolor{Gray}89.59 & \cellcolor{Gray}92.00 & 89.86                         & 92.93                         \\ \hline
seq10    & \cellcolor{Gray}92.20 & \cellcolor{Gray}94.06 & \cellcolor{Gray}90.47 & \cellcolor{Gray}92.75 & \cellcolor{Gray}90.05 & \cellcolor{Gray}92.84 \\ \hline
seq20    & 92.53                                                & 94.32                         & \cellcolor{Gray}90.95 & \cellcolor{Gray}93.08 & 89.02                         & 92.00                         \\ \Xhline{2.5\arrayrulewidth}
\end{tabular}}
\end{minipage}

\vskip 30pt
\begin{itemize}
    \item synthetic word order errors in training, especially with sequential training, have a \textbf{small positive effect} on actual learner sentences 
    \item \textbf{larger improvement} on (in-domain) corrupted sentences
    \item \textbf{minimal decline} on standard language
\end{itemize}
\vskip 40pt

\begin{minipage}{\linewidth}
    \centering
    \resizebox{0.29\columnwidth}{!}{
    \begin{tabular}{lcc}
\multicolumn{1}{c}{\textbf{}} & \textbf{LAS}                 & \textbf{UAS}                  \\ \Xhline{2.5\arrayrulewidth}
baseline             & {82.80} & {86.02}  \\ \Xhline{2.5\arrayrulewidth}
mix15                & 84.41                        & 89.25                         \\ \hline
mix50                & 87.10                        & 90.32                         \\ \hline
seq10                & 87.10                        & 89.78                         \\ \hline
seq20                & 86.02                        & \cellcolor{Gray}89.78 \\ \Xhline{2.5\arrayrulewidth}
\end{tabular}
    }
\end{minipage}
\vskip 30pt
\begin{itemize}
    \item \textbf{targeted evaluation} isolating erroneous segments from SweLL sentences shows a \textbf{wider performance gap} between baseline and specialized models
\end{itemize}
}

\block{Future Work}{
\begin{itemize}
    \item improving the \textbf{corruption pipeline}:
    \begin{itemize}
        \item more realistic S-WO errors
        \item support for more error types
    \end{itemize}
    \item corrupting texts closer to the \textbf{L2 domain}, such as coursebook materials
    \item rerunning the evaluation on a \textbf{larger test set} (possibly including other error types) and/or repeat experiments and compute \textbf{multi-run averages}
\end{itemize}
}

%\block{Resources}{
%\begin{itemize}
%    \item E. Volodina et al. \textit{{SweLL}-gold}. Distributed via SBX/CLARIN. 2022.
%    \item M. Malmsten et al. \textit{Swedish BERT models}. National Library of Sweden/KBLab. Distributed via HuggingFace. 2020.
%    \item J. Nivre and A. Smith. \textit{Swedish-Talbanken-UD}. Universal Dependencies \newline Consortium. Distributed via LINDAT/CLARIAH-CZ. 2023.
%\end{itemize}
%}

\end{columns}

\block{Acknowledgements}{
This work is supported by the Swedish national research infrastructure, funded jointly by the Swedish Research Council and the 10 participating partner institutions.
}

\end{document}

